{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ThinFilmDataCreation as tfdc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "import dask\n",
    "import glob as glob\n",
    "from tifffile import imread, imshow\n",
    "import warnings\n",
    "import dask\n",
    "\n",
    "def attempt(Real_Data, Length, i, init= None, solver = 'cd', beta_loss = 'frobenius', iter = 500):\n",
    "    NMF_model = NMF(n_components=i, init = init, solver = solver, beta_loss = beta_loss, max_iter = iter)\n",
    "    NMF_data= NMF_model.fit_transform(Real_Data)\n",
    "    fit_compos = NMF_model.components_\n",
    "    Q = np.array(NMF_model.reconstruction_err_)*100\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    return Q\n",
    "\n",
    "def attempt2(Real_Data, Length, i, init= None, solver = 'cd', beta_loss = 'frobenius', iter = 500):\n",
    "    NMF_model = NMF(n_components=i, init = init, solver = solver, beta_loss = beta_loss, max_iter = iter)\n",
    "    NMF_data= NMF_model.fit_transform(Real_Data)\n",
    "    fit_compos = NMF_model.components_\n",
    "    Q = np.array(NMF_model.reconstruction_err_)*100\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    return Q, fit_compos, NMF_data\n",
    "\n",
    "\n",
    "def Run_NMF(Real_Data, init= None, solver = 'cd', beta_loss = 'frobenius', itear = 1000, show = False):\n",
    "    \n",
    "    \"\"\"\" Comparison of multiple components without manually comparing\n",
    "    multiple components all at once. There are a few ways that this can be done. One way\n",
    "    is to take the residuals of the datasets with themselves to see which is the closests to the \n",
    "    \"correct\" component. This may only be done on sample data potentially. Could include a \n",
    "    simulated dataset of what the standing component should look like give or take some \n",
    "    variations in the data. When the residuals are taken if it is less than some given \n",
    "    percentage the would be the dataset/NMF component to use further. There may be multiple\n",
    "    thus having to go in manually to find differences. \n",
    "    \n",
    "    To do this, will need to import the simulated XRD pattern from VESTA and then take the residual differences\n",
    "    of each component and compared to the simulated pattern. Will have the program spit out the compnents that\n",
    "    meet the cutoff. \"\"\"\n",
    "\n",
    "    In = init\n",
    "    Solve = solver\n",
    "    Beta = beta_loss\n",
    "    It = itear\n",
    "    \n",
    "\n",
    "    jobs = [dask.delayed(attempt)(Real_Data, Real_Data.shape[1], i, In, Solve, Beta, It) for i in range(1, Real_Data.shape[1])]\n",
    "    #jobs = [dask.delayed(attempt)(Real_Data, Real_Data.shape[1], i, In, Solve, Beta, It) for i in range(1,20)]\n",
    "    calcs = dask.compute(jobs)[0]\n",
    "    print(calcs)\n",
    "\n",
    "    calcs = np.array(calcs)\n",
    "    min_Q = np.min(calcs)\n",
    "    noc = np.where(calcs == min_Q)\n",
    "    noc_2 = noc[0]\n",
    "    number_of_components = noc_2[0] +1\n",
    "\n",
    "    Divergence, compos, NMF_Data_2 = attempt2(Real_Data, Real_Data.shape[1], number_of_components, In, Solve, Beta)\n",
    "            \n",
    "    \n",
    "    print('The beta-divergence between the training data and reconstructed data is',\n",
    "             Divergence,'%' 'The final number of components used were',number_of_components+1)\n",
    "    \n",
    "    \n",
    "    m = pd.DataFrame(compos)\n",
    "    m = m.T\n",
    "    \n",
    "    if show == True:\n",
    "        plt.figure(figsize = (5,5))\n",
    "        colors = plt.cm.magma(np.linspace(0,1, number_of_components))\n",
    "        i = 0\n",
    "        while i < number_of_components:\n",
    "            plt.plot(m[i], c = colors[i], alpha = 0.7)\n",
    "            i = i+1\n",
    "    \n",
    "    return m,NMF_Data_2, min_Q\n",
    "\n",
    "def AggCluster(Number_Clusters, data):\n",
    "    \n",
    "    \"\"\"A program that will take in the type of scikitlearn clustering algorithm\n",
    "        desired and the number of clusters as well as the data in a numpy array\n",
    "        and output the associated clusters with the original data. This will make\n",
    "        the 'latent' space from the clustering algorithms have more meaning\"\"\"\n",
    "\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    Make_Clusters= AgglomerativeClustering(n_clusters = Number_Clusters, compute_distances=True)\n",
    "    y_kmeans = Make_Clusters.fit_predict(data)\n",
    "    information = Make_Clusters.fit(data)\n",
    "    parameter = information.distances_\n",
    "\n",
    "    x = 0\n",
    "    Understanding_data = {\"Cluster_Number\":[], \"Int_Angle\":[]};\n",
    "    while x < len(data):\n",
    "        Understanding_data[\"Cluster_Number\"].append(y_kmeans[x])\n",
    "        Understanding_data[\"Int_Angle\"].append(data[x])\n",
    "        x = x+1\n",
    "        \n",
    "    # Create an empty list to store the data\n",
    "    data_list = []\n",
    "    \n",
    "        \n",
    "    q = 0\n",
    "    while q < Number_Clusters:\n",
    "        z = 0\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Agglomerative Clustering'+' ' +str(q))\n",
    "\n",
    "        while z < len(data):\n",
    "            if Understanding_data[\"Cluster_Number\"][z] == q:\n",
    "                plt.plot(Understanding_data[\"Int_Angle\"][z], label = 'Component'+str(z))\n",
    "            z = z+1\n",
    "    \n",
    "        # Check if the plot looks good if the plot looks good, append the data to the list\n",
    "        plt.show(block = False)\n",
    "        plt.pause(0.1)\n",
    "        \n",
    "        if input(\"Do the identified components look like an isotropic scattering signal? (y/n)\") == 'y':\n",
    "            i = 0\n",
    "            while i < len(data):\n",
    "                if Understanding_data[\"Cluster_Number\"][i] == q:\n",
    "                    data_list.append(Understanding_data[\"Int_Angle\"][i])\n",
    "                i = i+1\n",
    "                \n",
    "        q = q+1\n",
    "    \n",
    "    return Understanding_data, data_list\n",
    "\n",
    "\n",
    "def smooth_components(Identified_components, filter_strength = 2, show = False):\n",
    "    '''A function that will smooth the components identified from the agglomerative clustering algorithm'''\n",
    "    \n",
    "    # Importing required library\n",
    "    from scipy.signal import savgol_filter\n",
    "    \n",
    "    # Defining a dictionary that maps filter strength to the number of points for the smoothing window\n",
    "    strength_to_points = {1: 3, 2: 5, 3: 7, 4: 11, 5: 15}\n",
    "    \n",
    "    # Retrieving the number of points for the smoothing window based on the filter strength provided\n",
    "    points = strength_to_points.get(filter_strength)\n",
    "\n",
    "    # Applying Savitzky-Golay filter to smooth the identified components by taking their average along the columns\n",
    "    smoothed_compos = savgol_filter(np.mean(Identified_components, axis = 0), points, 1)\n",
    "    \n",
    "    # If show argument is True, plotting the original components, their average, and the smoothed components\n",
    "    if show == True:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "        ax1.plot(Identified_components.T, c = 'k', linewidth = 2, alpha = 0.7)\n",
    "        ax1.set_title('Identified Components')\n",
    "        ax1.set_ylabel('Intensity')\n",
    "        ax2.plot(np.mean(Identified_components, axis = 0), c = 'r', linewidth = 2, alpha = 0.7)\n",
    "        ax2.set_ylabel('Intensity')\n",
    "        ax2.set_title('Mean of Identified Components')\n",
    "        ax3.plot(smoothed_compos, c = 'g', linewidth = 2, alpha = 0.7, label = 'Smoothed Component')\n",
    "        ax3.scatter(np.arange(len(np.mean(Identified_components, axis = 0))), np.mean(Identified_components, axis = 0), c = 'r', s = 10, \n",
    "                    label = 'Mean Component')\n",
    "        ax3.set_xlabel('Data Points')\n",
    "        ax3.set_title('Smoothed Component')\n",
    "        ax3.set_ylabel('Intensity')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # Returning the smoothed components\n",
    "    return smoothed_compos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nmfac(Data, initialize_iter = 0, clusters = 5):\n",
    "    \"\"\"A function that will run the NMF algorithm and then cluster with agglomerative clustering the components and returns the \n",
    "        identified components for later PDF analysis. The function starts with a random initializer\n",
    "        that will be used to initialize the NMF algorithm. The user can decide how many iterations the \n",
    "        initializer takes. Then it will go through the NMF algorithm and compare the beta divergences of all the\n",
    "        initializations and select the one with the lowest. By default, there is no initializer and the NMF algorithm\n",
    "        uses preset parameters to run the algorithm. \"\"\"\n",
    "        \n",
    "        #NMF Parameter Values\n",
    "    init_params = ['random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom']\n",
    "    solver_params = ['cd', 'mu']\n",
    "    beta_loss_params = ['frobenius', 'kullback-leibler', 'itakura-saito']\n",
    "    tol_params = np.arange(0.00001, 0.01, 0.0001)\n",
    "    max_iter_params = np.arange(100, 10000, 100)\n",
    "    shuffle_params = [True, False]\n",
    "        \n",
    "    #When random initializers are not wanted\n",
    "    if initialize_iter == 0:\n",
    "        weights, components, beta = Run_NMF(Data, show = True)\n",
    "        AggComponents = np.array(components).T\n",
    "        AggClusters, found_compos = AggCluster(clusters, AggComponents)\n",
    "        \n",
    "        found_compos = np.array(found_compos)\n",
    "        \n",
    "    elif initialize_iter > 0:\n",
    "        beta_div = []\n",
    "        \n",
    "        init_for_init = np.random.choice(init_params,initialize_iter)\n",
    "        solver_for_init = np.random.choice(solver_params, initialize_iter)\n",
    "        beta_loss_for_init = np.random.choice(beta_loss_params, initialize_iter)\n",
    "        tol_for_init = np.random.choice(tol_params, initialize_iter)\n",
    "        max_iter_for_init = np.random.choice(max_iter_params, initialize_iter)\n",
    "        shuffle_for_init = np.random.choice(shuffle_params, initialize_iter)\n",
    "        \n",
    "        for i in range(initialize_iter):   \n",
    "            weights, components, beta = Run_NMF(Data, init = init_for_init[i], solver = solver_for_init[i], \n",
    "                                                beta_loss = beta_loss_for_init[i], itear = max_iter_for_init[i], show = True)\n",
    "            \n",
    "            beta_div.append(beta)\n",
    "        beta_np = np.array(beta_div)\n",
    "        good_init = np.argmin(beta_np)\n",
    "        \n",
    "        weights, components, beta = Run_NMF(Data, show = True, init = init_for_init[good_init], solver = solver_for_init[good_init], beta_loss=beta_loss_for_init[good_init],\n",
    "                                             itear = max_iter_for_init[good_init])\n",
    "        \n",
    "        AggComponents = np.array(components).T\n",
    "        AggClusters, found_compos = AggCluster(clusters, AggComponents)\n",
    "        \n",
    "        found_compos = np.array(found_compos)\n",
    "            \n",
    "        \n",
    "    return found_compos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random' 'nndsvdar' 'nndsvd' 'nndsvdar' 'custom' 'random' 'custom']\n"
     ]
    }
   ],
   "source": [
    "init_params = ['random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom']\n",
    "solver_params = ['cd', 'mu']\n",
    "beta_loss_params = ['frobenius', 'kullback-leibler', 'itakura-saito']\n",
    "tol_params = np.random.choice(np.arange(0.00001, 0.01, 0.0001))\n",
    "max_iter_params = np.random.choice(np.arange(100, 10000, 100))\n",
    "shuffle_params = [True, False]\n",
    "\n",
    "for_init = np.random.choice(init_params, 7)\n",
    "\n",
    "print(for_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = [4, 5, 6, 0, 1, 2, 3]\n",
    "\n",
    "a = np.array(a)\n",
    "\n",
    "good_init = np.argmin(a)\n",
    "\n",
    "print(good_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_working",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
