{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import dask\n",
    "import glob as glob\n",
    "from tifffile import imread, imshow\n",
    "import warnings\n",
    "import os\n",
    "import dask\n",
    "\n",
    "\n",
    "def attempt(Real_Data, Length, i, init= None, solver = 'cd', beta_loss = 'frobenius', iter = 500):\n",
    "    # Create an NMF model with specified parameters\n",
    "    NMF_model = NMF(n_components=i, init = init, solver = solver, beta_loss = beta_loss, max_iter = iter)\n",
    "    \n",
    "    # Fit the NMF model to the input data\n",
    "    NMF_data= NMF_model.fit_transform(Real_Data)\n",
    "    \n",
    "    # Extract the factorized matrix components\n",
    "    fit_compos = NMF_model.components_\n",
    "    \n",
    "    # Calculate the reconstruction error of the model\n",
    "    Q = np.array(NMF_model.reconstruction_err_)*100\n",
    "    \n",
    "    # Suppress warnings from the library\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Return the reconstruction error, factorized matrix components, and transformed data\n",
    "    return Q, fit_compos, NMF_data\n",
    "\n",
    "def Run_NMF(Real_Data, init= None, solver = 'cd', beta_loss = 'frobenius', itear = 1000):\n",
    "    \"\"\" \n",
    "    This function performs NMF (Non-negative Matrix Factorization) on the input data, Real_Data.\n",
    "    The function takes in the following parameters:\n",
    "    Real_Data: the input data to be factorized\n",
    "    init: the initial guess of the factorization (default is None)\n",
    "    solver: the solver to be used in the factorization (default is 'cd' - Coordinate Descent)\n",
    "    beta_loss: the beta-divergence loss function to be used (default is 'frobenius')\n",
    "    itear: the number of iterations to perform in the factorization (default is 1000)\n",
    "\n",
    "    The function returns a tuple of two items:\n",
    "        1. A pandas dataframe of the NMF components\n",
    "        2. The reconstructed NMF data\n",
    "        \n",
    "    The function uses Dask for parallel computing, and opens a web browser to display the Dask dashboard. \n",
    "    \"\"\"\n",
    "    import dask.delayed\n",
    "    import dask.compute\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from dask.distributed import Client\n",
    "    client = Client()\n",
    "    import webbrowser\n",
    "    webbrowser.open(client.dashboard_link)\n",
    "\n",
    "    # Initialize variables\n",
    "    In = init\n",
    "    Solve = solver\n",
    "    Beta = beta_loss\n",
    "    It = itear\n",
    "\n",
    "    # Delayed computation of the function attempt for each number of components\n",
    "    jobs = [dask.delayed(attempt)(Real_Data, Real_Data.shape[1], i, In, Solve, Beta, It) for i in range(1, Real_Data.shape[1])]\n",
    "    calcs = dask.compute(jobs)[0]\n",
    "\n",
    "    # Convert the results to a numpy array\n",
    "    calcs = np.array(calcs)\n",
    "\n",
    "    # Find the minimum beta-divergence between the training data and the reconstructed data\n",
    "    min_Q = np.min(calcs)\n",
    "    noc = np.where(calcs == min_Q)\n",
    "    noc_2 = noc[0]\n",
    "    number_of_components = noc_2[0] + 1\n",
    "\n",
    "    # Run the final NMF with the number of components found\n",
    "    Divergence, compos, NMF_Data_2 = attempt(Real_Data, Real_Data.shape[1], number_of_components, In, Solve, Beta)\n",
    "\n",
    "    # Print the beta-divergence and number of components used\n",
    "    print('The beta-divergence between the training data and reconstructed data is',\n",
    "            Divergence,'%', 'The final number of components used were', number_of_components + 1, '/n')\n",
    "\n",
    "    # Create a pandas dataframe of the NMF components\n",
    "    components_dataframe = pd.DataFrame(compos).T\n",
    "\n",
    "    # Plot the NMF components\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(components_dataframe, linewidth=2, alpha=0.7, c='k')\n",
    "    plt.title('NMF Components')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.xlabel('Q')\n",
    "\n",
    "    return components_dataframe, NMF_Data_2\n",
    "\n",
    "def AggCluster(Number_Clusters, data):\n",
    "    \n",
    "    \"\"\"\n",
    "    A program that will take in the type of scikitlearn clustering algorithm\n",
    "    desired and the number of clusters as well as the data in a numpy array\n",
    "    and output the associated clusters with the original data. This will make\n",
    "    the 'latent' space from the clustering algorithms have more meaning\n",
    "    \n",
    "    Input:\n",
    "    Number_Clusters - int, the number of clusters desired\n",
    "    data - numpy array, the data to be used for clustering\n",
    "    \n",
    "    Output:\n",
    "    Understanding_data - dictionary, contains the cluster number and the associated\n",
    "                        int_angle value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import the AgglomerativeClustering function from the scikit-learn library\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    \n",
    "    # Create an AgglomerativeClustering object with the desired number of clusters\n",
    "    Make_Clusters = AgglomerativeClustering(n_clusters=Number_Clusters, compute_distances=True)\n",
    "    \n",
    "    # Fit the model to the data and predict the cluster labels\n",
    "    y_kmeans = Make_Clusters.fit_predict(data)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    information = Make_Clusters.fit(data)\n",
    "    \n",
    "    # Get the distances parameter\n",
    "    parameter = information.distances_\n",
    "\n",
    "    # Initialize the variables for looping\n",
    "    x = 0\n",
    "    Understanding_data = {\"Cluster_Number\":[], \"Int_Angle\":[]}\n",
    "    \n",
    "    # Loop through each data point and add the cluster number and int_angle value to the dictionary\n",
    "    while x < len(data):\n",
    "        Understanding_data[\"Cluster_Number\"].append(y_kmeans[x])\n",
    "        Understanding_data[\"Int_Angle\"].append(data[x])\n",
    "        x = x+1\n",
    "        \n",
    "    # Loop through each cluster and create a separate plot for each cluster\n",
    "    q = 0\n",
    "    while q < Number_Clusters:\n",
    "        z = 0\n",
    "        plt.figure()\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Agglomerative Clustering' + ' ' + str(q))\n",
    "\n",
    "        # Loop through each data point and plot only the points belonging to the current cluster\n",
    "        while z < len(data):\n",
    "            if Understanding_data[\"Cluster_Number\"][z] == q:\n",
    "                plt.plot(Understanding_data[\"Int_Angle\"][z], label='Component' + str(z))\n",
    "                \n",
    "            z = z + 1\n",
    "        q = q + 1\n",
    "        \n",
    "    # Return the dictionary of cluster numbers and int_angle values\n",
    "    \n",
    "    return Understanding_data\n",
    "\n",
    "def Combine_Cluster(Understanding_data):\n",
    "    \"\"\"\n",
    "        A program that will take in a user specified cluster or clusters of choice, from the Agglomerative \n",
    "        cluster function and average the signals for one array of data. This data will then be\n",
    "        smoothed in a different function called smooth_data.\n",
    "        \n",
    "        Input: \n",
    "        Understanding_data - dictionary, contains the cluster number and the associated\n",
    "        \n",
    "        Output:\n",
    "        Identified_signal - averaged numpy array, the averaged signal from the cluster of interest\n",
    "    \"\"\"\n",
    "    cluster_of_interest = []\n",
    "    keep_going = True\n",
    "    cluster_numbers = np.array(Understanding_data['Cluster_Number'])\n",
    "    \n",
    "    while keep_going == True:\n",
    "        print('Please enter the cluster number of interest')\n",
    "        for i in range(0, len(cluster_numbers)):\n",
    "            print('Cluster ', i+1)\n",
    "        selected_cluster = int(input(\"Enter the cluster number of interest: \")) - 1\n",
    "        cluster = np.where(cluster_numbers == selected_cluster)\n",
    "        cluster_data = cluster[0]\n",
    "        cluster_of_interest.append(cluster_data)\n",
    "        keep_going = bool(input(\"Would you like to add another cluster? (True/False)\"))\n",
    "    \n",
    "    arrayed_cluster = np.array(cluster_of_interest)\n",
    "    Identified_signal = np.mean(arrayed_cluster, axis = 0)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.plot(Identified_signal)\n",
    "    plt.x_label('Q')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.title('Averaged Signal')\n",
    "    \n",
    "    return Identified_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research_Sort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f4f283a4dd7a5c767cc0b66dd63473be29edef97eb0671f08a1190fe6eb66f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
